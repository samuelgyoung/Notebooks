{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4 - Fundamentals of machine learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Four branches of machine learning</h2>\n",
    "\n",
    "- Supervised learning - Generally, almost all applications of deep learning that are in the spotlight these days belong in this category, such as optical character recognition, speech recognition, image classification, and language translation.\n",
    "\n",
    "classification \n",
    "regression\n",
    "Sequence generation - Given a picture, predict a caption describing it. \n",
    "Syntax tree prediction - Given a sentence, predict its decomposition into a syntax tree.\n",
    "Object detection - Given a picture, draw a bounding box around certain objects inside the picture.\n",
    "Image segmentation - Given a picture, draw a pixel-level mask on a specific object.\n",
    "\n",
    "\n",
    "- Unsupervised learning\n",
    "\n",
    "Finding interesting transformations of the input data without the help of any targets. Dimensionality reduction and clustering \n",
    "\n",
    "- Self-supervised learning\n",
    "\n",
    "Subset of supervised learning. Self-supervised learning is supervised learning without human-annotated labels. you can think of it as supervised learning without any humans in the loop. Example: trying to pre- dict the next frame in a video, given past frames, or the next word in a text, given previ- ous words\n",
    "\n",
    "- Reinforcement learning\n",
    "\n",
    "an agent receives information about its environment and learns to choose actions that will maximize some reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Regression definitions:\n",
    "    \n",
    " Sample or input—One data point that goes into your model.\n",
    "\n",
    " Prediction or output—What comes out of your model.\n",
    "\n",
    " Target—The truth. What your model should ideally have predicted, according\n",
    "to an external source of data.\n",
    "\n",
    " Prediction error or loss value—A measure of the distance between your model’s prediction and the target.\n",
    "\n",
    " Classes—A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, “dog” and “cat” are the two classes.\n",
    "\n",
    " Label—A specific instance of a class annotation in a classification problem. For instance, if picture #1234 is annotated as containing the class “dog,” then “dog” is a label of picture #1234.\n",
    "\n",
    " Ground-truth or annotations—All targets for a dataset, typically collected by humans.\n",
    "\n",
    " Binary classification—A classification task where each input sample should be categorized into two exclusive categories.\n",
    "\n",
    " Multiclass classification—A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.\n",
    "\n",
    "Multilabel classification—A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the “dog” label. The number of labels per image is usually variable.\n",
    "\n",
    " Scalar regression—A task where the target is a continuous scalar value. Pre- dicting house prices is a good example: the different target prices form a con- tinuous space.\n",
    "\n",
    " Vector regression—A task where the target is a set of continuous values: for example, a continuous vector. If you’re doing regression against multiple val- ues (such as the coordinates of a bounding box in an image), then you’re doing vector regression.\n",
    "\n",
    " Mini-batch or batch—A small set of samples (typically between 8 and 128) that are processed simultaneously by the model. The number of samples is often a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch is used to compute a single gradient-descent update applied to the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Evaluating a Model</h2>\n",
    "\n",
    "Train, Validate (feedback to improve), and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>With Little Data</h3>\n",
    "\n",
    "simple hold-out validation, K- fold validation, and iterated K-fold validation with shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple hold-out Validation:\n",
    "\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. As you saw in the previous sections, in order to prevent infor- mation leaks, you shouldn’t tune your model based on the test set, and therefore you should also reserve a validation set.\n",
    "\n",
    "Flaw: if little data is available, then your validation and test sets may contain too few samples to be statisti- cally representative of the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-15aa604807f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_validation_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "num_validation_samples = 10000 \n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_data = data[:num_validation_samples]\n",
    "\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:]\n",
    "\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "\n",
    "#Once you’ve tuned your hyperparameters, it’s common to train your final model from scratch on all non-test data available.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "\n",
    "\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-FOLD VALIDATION</h3>\n",
    "\n",
    "split your data into K partitions of equal size. For each parti- tion i, train a model on the remaining K – 1 partitions, and evaluate it on partition i. Your final score is then the averages of the K scores obtained. \n",
    "\n",
    "Pro: when the performance of your model shows significant variance based on your train- test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6ba662902ca0>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6ba662902ca0>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    validation_scores = [] for fold in range(k):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = [] for fold in range(k):\n",
    "    \n",
    "    #Selects the validation- data partition\n",
    "    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)]\n",
    "    \n",
    "    #Uses the remainder of the data as training data. Note that the + operator is list concatenation, not summation.\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    \n",
    "    #Creates a brand-new instance of the model (untrained)\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data) \n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "#Validation score: average of the validation scores of the k folds\n",
    "validation_score = np.average(validation_scores)\n",
    "\n",
    "#Trains the final model on all non- test data available\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ITERATED K-FOLD VALIDATION WITH SHUFFLING</h3>\n",
    "\n",
    "relatively little data available and you need to evaluate your model as precisely as possible.\n",
    "\n",
    "applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating P × K models (where P is the number of iterations you use), which can very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pointers</h3>\n",
    "\n",
    "Randomly Shuffle Data : Don't have image sets of 1-7s in training and 8-9 in test\n",
    "\n",
    "Arrow of time: Don't shuffle time data, make sure test data is at the end of the timeline\n",
    "\n",
    "Redundancy: Make sure you don't have the same data in train and test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Data preprocessing, FE and Feature learning<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preprocessing for neural networks</h3>\n",
    "\n",
    "-raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>VECTORIZATION</h4>\n",
    "\n",
    "-All inputs and targets in a neural network must be tensors of floating-point data \n",
    "-turn into tensors, a step called data vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>VALUE NORMALIZATION</h4>\n",
    "\n",
    "In general, it isn’t safe to feed into a neural network data that takes relatively large val- ues (for example, multidigit integers, which are much larger than the initial values taken by the weights of a network) or data that is heterogeneous (for example, data where one feature is in the range 0–1 and another is in the range 100–200). Doing so can trigger large gradient updates that will prevent the network from converging. To make learning easier for your network, your data should have the following characteristics:\n",
    "\n",
    "Take small values—Typically, most values should be in the 0–1 range.\n",
    "\n",
    "Be homogenous—That is, all features should take values in roughly the same range.\n",
    "\n",
    "Additionally, the following stricter normalization practice is common and can help. (wouldn't do this in digit classification)\n",
    "\n",
    "Normalize each feature independently to have a mean of 0.\n",
    "\n",
    "Normalize each feature independently to have a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dff11f5e8361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Assuming x is a 2D data matrix of shape (samples, features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#This is easy to do with Numpy arrays:\n",
    "\n",
    "#Assuming x is a 2D data matrix of shape (samples, features)\n",
    "x -= x.mean(axis=0) \n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>HANDLING MISSING VALUES</h4>\n",
    "\n",
    "In general, with neural networks, it’s safe to input missing values as 0, with the con- dition that 0 isn’t already a meaningful value. The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature engineering</h4>\n",
    "\n",
    "modern deep learning removes the need for most feature engineer- ing, because neural networks are capable of automatically extracting useful features from raw data.\n",
    "\n",
    "Still important:\n",
    "    \n",
    "    - Good features still allow you to solve problems more elegantly while using fewer resources.\n",
    "    - Good features let you solve a problem with far less data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overfitting and underfitting</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental issue in machine learning is the tension between optimization and generalization.\n",
    "\n",
    "If model is giving illrelevant patterns:\n",
    "- the best solution is to get more training data.\n",
    "- modulate the quantity of information that your model is allowed to store or to add constraints on what information it’s allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
    "- Preventing overfitting is called regularization. Some techniques:\n",
    "\n",
    "\n",
    "<h3> 1. Reduce the network size: </h3>\n",
    "\n",
    "\n",
    "- determined by the number of layers and the number of units per layer (these are known as capacity)\n",
    "- The more capacity the network has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large differ- ence between the training and validation loss).\n",
    "\n",
    "<h3> 2. Adding weight regularization </h3>\n",
    "\n",
    "- Simpler models are less likely to over- fit than complex ones.\n",
    "-  mitigate overfitting is to put constraints on the complex- ity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. (weight regularization)\n",
    "\n",
    "\n",
    "two flavors:\n",
    "\n",
    "L1 regularization—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).\n",
    "\n",
    "L2 regularization—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights). L2 regularization is also called weight decay in the context of neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-86dc8cb97d1a>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-86dc8cb97d1a>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    activation='relu')) model.add(layers.Dense(1, activation='sigmoid'))\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Example: Let’s add L2 weight regularization to the movie-review classifi- cation network.\n",
    "\n",
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "    activation='relu', input_shape=(10000,))) \n",
    "\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "    activation='relu')) \n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#l2(0.001) means every coefficient in the weight matrix of the layer will\n",
    "# add 0.001 * weight_coefficient_value to the total loss of the network.\n",
    "\n",
    "#Alternative alternative to L2 regularization, you can use one of the following Keras weight regularizers.\n",
    "\n",
    "from keras import regularizers \n",
    "\n",
    "regularizers.l1(0.001) #L1 regularization\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) #Simultaneous L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding dropout</h3>\n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization tech- niques for neural networks.\n",
    "\n",
    "Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. \n",
    "\n",
    "In Keras, you can introduce dropout in a network via the Dropout layer, which is applied to the output of the layer right before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7384186fe9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ADDING TWO DROPOUTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#ADDING TWO DROPOUTS\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See 4.5 for outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
